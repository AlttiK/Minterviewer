# ğŸ¨ Architecture Overview

## System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Chrome Browser                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  Chrome Extension (Manifest V3)                    â”‚     â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     â”‚
â”‚  â”‚  â”‚  Popup UI (HTML/CSS/JS)                      â”‚  â”‚     â”‚
â”‚  â”‚  â”‚  â€¢ Interview settings                        â”‚  â”‚     â”‚
â”‚  â”‚  â”‚  â€¢ Chat interface                            â”‚  â”‚     â”‚
â”‚  â”‚  â”‚  â€¢ Audio player                              â”‚  â”‚     â”‚
â”‚  â”‚  â”‚  â€¢ Feedback display                          â”‚  â”‚     â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†• HTTP/JSON
                    (localhost:8000)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FastAPI Backend                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  API Endpoints                                     â”‚     â”‚
â”‚  â”‚  â€¢ POST /chat - Send messages                     â”‚     â”‚
â”‚  â”‚  â€¢ POST /feedback - Get feedback                  â”‚     â”‚
â”‚  â”‚  â€¢ GET /health - Health check                     â”‚     â”‚
â”‚  â”‚  â€¢ GET /audio/* - Serve audio files               â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                            â†•                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  Session Manager                                   â”‚     â”‚
â”‚  â”‚  â€¢ In-memory storage                               â”‚     â”‚
â”‚  â”‚  â€¢ Message history                                 â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†• HTTP                              â†• Shell
    (localhost:11434)                    (piper command)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Ollama Service     â”‚            â”‚    Piper TTS         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  LLM Model     â”‚  â”‚            â”‚  â”‚  Voice Model   â”‚  â”‚
â”‚  â”‚  (llama3.2)    â”‚  â”‚            â”‚  â”‚  (en_US)       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â€¢ Generate replies  â”‚            â”‚  â€¢ Text â†’ Speech   â”‚  â”‚
â”‚  â€¢ Context aware     â”‚            â”‚  â€¢ WAV output      â”‚  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Data Flow

### 1. Start Interview
```
User clicks "Start Interview"
    â†“
Extension sends POST /chat with system prompt
    â†“
Backend forwards to Ollama
    â†“
Ollama generates first question
    â†“
Backend receives AI response
    â†“
Backend calls Piper TTS
    â†“
Piper generates speech.wav
    â†“
Backend returns { message, audio_url }
    â†“
Extension displays text + plays audio
```

### 2. User Responds
```
User types answer and clicks "Send"
    â†“
Extension sends POST /chat with conversation history
    â†“
Backend retrieves session from memory
    â†“
Backend forwards to Ollama with full context
    â†“
Ollama generates contextual response
    â†“
Backend generates TTS audio
    â†“
Backend returns response + audio
    â†“
Extension updates chat UI
```

### 3. End Interview
```
User clicks "End Interview"
    â†“
Extension sends POST /feedback
    â†“
Backend adds feedback prompt to messages
    â†“
Ollama analyzes full conversation
    â†“
Ollama generates feedback (strengths/weaknesses/tips)
    â†“
Backend parses feedback
    â†“
Backend returns structured feedback
    â†“
Extension displays feedback summary
```

## Component Interactions

```
Extension Components:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  popup.html     â”‚ â†’ User Interface (HTML/CSS)
â”‚  popup.js       â”‚ â†’ Logic & API calls
â”‚  manifest.json  â”‚ â†’ Extension config
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Backend Components:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  app.py         â”‚ â†’ FastAPI app + endpoints
â”‚  tts_helper.py  â”‚ â†’ TTS utilities
â”‚  sessions {}    â”‚ â†’ In-memory storage
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

External Services:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ollama         â”‚ â†’ AI model runtime
â”‚  Piper          â”‚ â†’ TTS engine
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Technology Stack

| Layer | Technology | Purpose |
|-------|-----------|---------|
| **Frontend** | HTML/CSS/JS | Extension UI |
| **Extension** | Chrome Manifest V3 | Extension framework |
| **API** | FastAPI | Web server |
| **Runtime** | Uvicorn | ASGI server |
| **AI** | Ollama | LLM runtime |
| **Model** | Llama 3.2 | Language model |
| **TTS** | Piper | Speech synthesis |
| **HTTP** | httpx | Async HTTP client |
| **Data** | In-memory dict | Session storage |

## File Dependencies

```
Chrome Extension
â”œâ”€â”€ manifest.json (entry point)
â”œâ”€â”€ popup.html (loaded by manifest)
â””â”€â”€ popup.js (loaded by popup.html)

Backend
â”œâ”€â”€ app.py (entry point)
â”œâ”€â”€ requirements.txt (dependencies)
â””â”€â”€ tts_helper.py (imported by app.py)

Generated at Runtime
â””â”€â”€ audio/
    â””â”€â”€ speech_*.wav (TTS outputs)
```

## Network Ports

```
localhost:8000  â†’ FastAPI backend
localhost:11434 â†’ Ollama API
```

## Security Model

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Chrome Extension Security          â”‚
â”‚  â€¢ Runs in isolated sandbox         â”‚
â”‚  â€¢ Can only connect to localhost    â”‚
â”‚  â€¢ No external network access       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“ CORS (localhost only)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI Backend Security           â”‚
â”‚  â€¢ CORS allows all origins (local)  â”‚
â”‚  â€¢ No authentication required       â”‚
â”‚  â€¢ Binds to 0.0.0.0:8000            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“ HTTP (localhost)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ollama Service                     â”‚
â”‚  â€¢ Runs locally only                â”‚
â”‚  â€¢ No internet access needed        â”‚
â”‚  â€¢ All data stays on machine        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Session Management

```
Session Lifecycle:

1. START
   User clicks "Start Interview"
      â†“
   Generate session_id (timestamp)
      â†“
   Create sessions[session_id] = []

2. ACTIVE
   Each chat message:
      â†“
   Append to sessions[session_id]
      â†“
   Send to Ollama with full history
      â†“
   Append AI response to session

3. END
   User clicks "End Interview"
      â†“
   Request feedback with full history
      â†“
   Session remains in memory until server restart
```

## Error Handling Flow

```
Extension Error Handling:
try {
    fetch(API_ENDPOINT)
} catch {
    Display error in UI
    Keep UI responsive
}

Backend Error Handling:
try {
    await ollama_api()
} catch OllamaConnectionError {
    Return 503 with helpful message
} catch Exception {
    Return 500 with error details
}

TTS Error Handling:
try {
    generate_tts()
} catch PiperNotFound {
    Return response without audio
    Log warning
} catch Exception {
    Continue without TTS
}
```

## Performance Characteristics

| Operation | Time | Notes |
|-----------|------|-------|
| Extension load | ~50ms | One-time |
| Start interview | 2-5s | Ollama model load |
| Chat response | 1-3s | Depends on model size |
| TTS generation | 0.5-1s | Per response |
| Audio playback | Varies | Depends on response length |
| Feedback generation | 2-4s | Analyzes full conversation |

## Resource Usage

| Component | RAM | Disk | CPU |
|-----------|-----|------|-----|
| Chrome Extension | ~10MB | ~30KB | Minimal |
| FastAPI Backend | ~50MB | ~10KB | Low |
| Ollama (idle) | ~100MB | ~2-4GB | Low |
| Ollama (active) | ~2-4GB | ~2-4GB | High |
| Piper TTS | ~50MB | ~5MB | Medium (burst) |

## Scalability Considerations

**Current Design (MVP):**
- âœ… Single user
- âœ… One interview at a time
- âœ… In-memory sessions
- âœ… Local execution

**To Scale (Future):**
- Add database (PostgreSQL/Redis)
- Add authentication (JWT)
- Add rate limiting
- Add session persistence
- Multi-user support
- Distributed deployment

## Extension vs Backend Responsibilities

| Responsibility | Extension | Backend |
|---------------|-----------|---------|
| UI rendering | âœ… | âŒ |
| User input | âœ… | âŒ |
| Audio playback | âœ… | âŒ |
| API calls | âœ… | âŒ |
| Session storage | âŒ | âœ… |
| AI communication | âŒ | âœ… |
| TTS generation | âŒ | âœ… |
| Business logic | âŒ | âœ… |

## Development Workflow

```
1. Modify Code
   â”œâ”€â”€ Extension: Reload in chrome://extensions
   â””â”€â”€ Backend: Auto-reload with --reload flag

2. Test Changes
   â”œâ”€â”€ Extension: Open popup and test
   â””â”€â”€ Backend: Run test_backend.py

3. Debug
   â”œâ”€â”€ Extension: Chrome DevTools (F12)
   â””â”€â”€ Backend: Terminal logs + /health endpoint

4. Deploy
   â”œâ”€â”€ Extension: Package and submit to Chrome Web Store
   â””â”€â”€ Backend: Docker + cloud hosting (if needed)
```

---

This architecture is designed to be:
- **Simple**: Minimal moving parts
- **Local**: No cloud dependencies
- **Fast**: Async operations throughout
- **Extensible**: Easy to add features
- **Debuggable**: Clear separation of concerns
